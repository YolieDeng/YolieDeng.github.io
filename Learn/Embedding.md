# Embedding 模型作用与原理

## 什么是Embedding
使用增强检索生成（RAG）技术来搭建本地知识库时，Embedding模型就是索引，指导快速定位到目标。

Embedding就是将数据数字化，把各种模态的数据（如：文本、图像、语音等）转换成数字，也就是向量。

Embedding向量本质上就是将语义信息映射到高维空间的数学坐标。

### 维度概念
- **低维（<100）**：语义区分能力弱，可能出现“苹果”（水果）与“苹果”（手机）混淆
- **高维（>1000）**：需要更多的计算资源，但能捕捉“跑步”与”慢跑“的细微差别

## Embedding模型原理

### 词向量化

#### 独热编码（one - hot encoding）
热独编码是一种将分类数据转换为机器学习算法可以更好理解的数值形式的编码方法。
> 基本原理：将每个分类值转换为一个二进制向量，向量的长度等于分类的总数，只有对应类别的位置是1，其他位置是0。
```
原始数据：
颜色
红
蓝
绿
蓝

转换后：
红  蓝  绿
1   0   0   (红)
0   1   0   (蓝)
0   0   1   (绿)
0   1   0   (蓝)
```
缺点：当类别太多的时候，会产生高维稀疏矩阵，占用内存空间，计算量大，且无法体现词与词之间的语义关系。

####  词嵌入（word embeddings）
词嵌入是一种将文本中的词语转换为密集的数值向量的技术，它能够捕捉词语之间的语义关系，像word2vec、glove等模型都属于词嵌入模型。
> 基本原理：将每个词映射到一个固定维度的实数向量空间，语义相近的词在这个向量空间中的距离也相近，通常是一个低维稠密向量（比如50维、100维、300维等）。

### 句子向量化

#### 简单平均/加权平均
简单加权就是把句子中每个词的向量加起来，然后除以词的数量，得到句子的向量。
加权平均则是根据词的重要性给每个词的向量赋上不同的权重，然后再进行运算。

#### 递归神经网络（RNN）
RNN 是一种专门处理序列数据的神经网络，具有"记忆"能力，可以记住之前的信息，同一个神经网络可以处理不同长度的序列。

```
输入："今天天气"
RNN预测下一个可能的词："晴"、"不错"、"真好"
原理：RNN记住了之前输入的文字，根据上下文预测最合适的下一个词
```

#### 卷积神经网络（CNN）
CNN是一种专门用于处理网格结构数据（如图像）的神经网络，主要特点是使用卷积运算来提取特征，能够自动学习图像的空间特征。

#### 自注意力机制（如Transformer）
自注意力机制能够直接计算序列中任意位置之间的关联，不像RNN需要按顺序处理，可以并行计算。"注意力"就像人类观察事物时的关注点分配。
```
核心组成：Query（查询）、Key（键）、Value（值）
例子：在图书馆找书
Query: 你想找的书的特征（如"Python编程"）
Key: 书架上每本书的标签
Value: 书架上的实际书籍
注意力分数 = Query和Key的相似度
最终输出 = 注意力分数加权的Value
```

