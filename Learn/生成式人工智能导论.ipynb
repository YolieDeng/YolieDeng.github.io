{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一讲 生成式AI是什么？\n",
    "\n",
    "### Generative AI:让机器产生负责又有结构的物件（如：文章、影像、语音）\n",
    "\n",
    "### 什么不是生成式AI？\n",
    "- 分类：从有限的选择中做选择（如：垃圾邮件侦测、猫狗分类器）\n",
    "\n",
    "![机器学习](img/生成式人工智能导论/1_1.png)\n",
    "### 机器学习：机器自动从资料找一个函式\n",
    "\n",
    "![生成式人工智能关系图](img/生成式人工智能导论/1_2.png)\n",
    "\n",
    "# 80分钟快速了解大语言模型————李宏毅\n",
    "\n",
    "![Token](img/生成式人工智能导论/1_3.png)\n",
    "\n",
    "### 语言模型在回答的时候都会掷骰子，如果只是按照概率或可能性最高最接近的答案来回答的效果不一定是最好的。可阅读[The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751# \"点击阅读论文\")\n",
    "![为什么要掷骰子](img/生成式人工智能导论/1_4.png)\n",
    "\n",
    "### 预训练 ———— 督导式学习 ———— 增强式学习\n",
    "（模型要有一定能力之后才能进入RLHF）\n",
    "![模型](img/生成式人工智能导论/1_7.png)\n",
    "\n",
    "督导式学习的重要性[InstructGPT](https://arxiv.org/abs/2203.02155 \"点击阅读论文\")，有督导式学习，小模型也有可能胜过大模型。\n",
    "\n",
    "### 增强式学习--模型回答问题的时候会给出两个不同的回答，当你告诉模型哪个答案更好的情况下，模型会去提高这个答案的几率。\n",
    "\n",
    "### ChatGPT的增强式学习\n",
    "![1.模仿人类老师的喜好](img/生成式人工智能导论/1_5.png)\n",
    "![2.向模拟老师学习](img/生成式人工智能导论/1_6.png)\n",
    "\n",
    "### 提高ChatGPT回复的准确率\n",
    "1. 把需求讲清楚\n",
    "2. 提供有关咨询给ChatGPT\n",
    "3. 提供范例\n",
    "4. 鼓励ChatGPT想一想[Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916 \"点击阅读论文\")\n",
    "5. 如何找出最优语句[Large Language Models as Optimizers](https://arxiv.org/abs/2309.03409 \"点击阅读论文\")\n",
    "6. 可以上传文件\n",
    "7. ChatGPT可以使用其他工具\n",
    "8. 拆解任务[Re3: Generating Longer Stories With Recursive Reprompting and Revision](https://arxiv.org/abs/2210.06774 \"点击阅读论文\")\n",
    "9. 自主进行规划[让AI自己增进自己的能力：Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073 \"点击阅读论文\")\n",
    "10. ChatGPT其实是会反省的[ERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents](https://arxiv.org/abs/2303.17071 \"点击阅读论文\")\n",
    "11. 跟真实环境互动[Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents](https://arxiv.org/abs/2201.07207 \"点击阅读论文\")[Inner Monologue:\n",
    "Embodied Reasoning through Planning with Language Models](https://innermonologue.github.io/ \"点击阅读论文\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二讲 今日的生成式AI厉害在哪？\n",
    "\n",
    "### ChatGPT能做什么？\n",
    "![ChatGPT能做什么？](img/生成式人工智能导论/2_1.png)\n",
    "\n",
    "### 这些人工智能在想什么？\n",
    "[Language Models Represent Space and Time](https://arxiv.org/abs/2310.02207 \"点击阅读论文\")\n",
    "\n",
    "### 改变自己来强化模型，如提示词工程\n",
    "\n",
    "### 训练自己的模型\n",
    "[Examining Forgetting in Continual Pre-training of Aligned Large Language Models](https://arxiv.org/abs/2401.03129 \"点击阅读论文\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三讲 训练不了人工智能，如何训练自己？\n",
    "## 注意：在本讲中，没有任何模型被训练\n",
    "\n",
    "### 对模型使用的神奇咒语\n",
    "\n",
    "#### 模型思考 Chain of Thought(CoT)\n",
    "[Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916 \"点击阅读论文\")\n",
    "\n",
    "#### 让模型解释一下自己的答案\n",
    "[Can Large Language Models Be an Alternative to Human Evaluations?](https://arxiv.org/abs/2305.01937 \"点击阅读论文\")\n",
    "[A Closer Look into Automatic Evaluation Using Large Language Models](https://arxiv.org/abs/2310.05657 \"点击阅读论文\")\n",
    "\n",
    "#### 对模型情绪勒索\n",
    "[Large Language Models Understand and Can be Enhanced by Emotional Stimuli](https://arxiv.org/abs/2307.11760 \"点击阅读论文\")\n",
    "\n",
    "#### 更多相关咨询\n",
    "[Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4](https://arxiv.org/abs/2312.16171 \"点击阅读论文\")\n",
    "\n",
    "#### 用AI来找神奇咒语（提升模型回答的准确率）\n",
    "- 用增强式学习（Reinforcement Learning，RL）[Learning to Generate Prompts for Dialogue Generation through Reinforcement Learning](https://arxiv.org/abs/2206.03931 \"点击阅读论文\")\n",
    "- 直接用语言模型来找[Large Language Models Are Human-Level Prompt Engineers](https://arxiv.org/abs/2211.01910 \"点击阅读论文\")\n",
    "\n",
    "### 神奇咒语不一定对所有模型都有用\n",
    "![叫模型思考不一定对所有模型都有效](img/生成式人工智能导论/3_1.png)\n",
    "[The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning](https://arxiv.org/abs/2205.03401 \"点击阅读论文\")\n",
    "\n",
    "### 在不训练模型的情况下强化语言模型的方法\n",
    "- 把前提讲清楚\n",
    "- 提供生成式AI原本不清楚的咨询\n",
    "- 提供范例 \n",
    "    [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165 \"点击阅读论文\")\n",
    "    [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837 \"点击阅读论文\")\n",
    "    [Larger language models do in-context learning differently](https://arxiv.org/abs/2303.03846 \"点击阅读论文\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四讲 训练不了人工智能，你可以训练自己\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
